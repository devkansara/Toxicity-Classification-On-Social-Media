{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# reading the data file\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# 6 class labels\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "# assignning comment and id to X\n",
    "cols = [0,1]\n",
    "X = data[data.columns[cols]]\n",
    "\n",
    "\n",
    "# assigning class-labels to Y\n",
    "cols1 = [2,3,4,5,6,7]\n",
    "Y = data[data.columns[cols1]]\n",
    "\n",
    "\n",
    "# splitting the data\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "# using tf-idf\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1,2), sublinear_tf=1, strip_accents='unicode', tokenizer=tokenize, \n",
    "                         min_df=3, max_df=0.9, use_idf=1, smooth_idf=1)\n",
    "\n",
    "# fit and transform after tf_idf\n",
    "train_tf_idf = tf_idf.fit_transform(X_train[\"comment_text\"])\n",
    "test_tf_idf = tf_idf.transform(X_test[\"comment_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains functions for ensemble learning\n",
    "\n",
    "x = train_tf_idf\n",
    "test_x = test_tf_idf\n",
    "\n",
    "# nb classifier\n",
    "def naive_bayes(y_i, y):\n",
    "    prob = x[y == y_i].sum(0)\n",
    "    return (prob + 1) / ( (y == y_i).sum() + 1 )\n",
    "\n",
    "# ensemble of naive-bayes and logistic regression classifiers\n",
    "def ensemble(y):\n",
    "    y = y.values\n",
    "    res = np.log(naive_bayes(1,y) / naive_bayes(0,y))\n",
    "    \n",
    "    classifier = LogisticRegression(C=4)\n",
    "    \n",
    "    nb = x.multiply(res)\n",
    "    \n",
    "    return classifier.fit(nb, y), res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for storing results\n",
    "preds = np.zeros((len(X_test), len(label_cols)))\n",
    "preds1 = np.zeros((len(X_test), len(label_cols)))\n",
    "\n",
    "# for loop for each class\n",
    "for i, j in enumerate(label_cols):\n",
    "    # print each class-label\n",
    "    print('fit', j)\n",
    "    \n",
    "    # calling ensemble function\n",
    "    m,r = ensemble(y_train[j])\n",
    "    \n",
    "    # generating outputs\n",
    "    preds1[:,i] = m.predict(test_x.multiply(r))\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the values\n",
    "y_train_new = y_train.values\n",
    "y_test_new = y_test.values \n",
    "y_score = preds\n",
    "\n",
    "# generating precision-recall curve\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "\n",
    "for i in range(6):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_new[:, i], y_score[:, i])\n",
    "    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n",
    "\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# generating roc curve and roc score\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_score=[]\n",
    "\n",
    "for i in range(6):\n",
    "    roc_score.append(roc_auc_score(y_test_new[:, i], y_score[:, i]))\n",
    "#     print(i,\" \",roc_auc_score(y_test_new[:, i], y_score[:, i]))\n",
    "    \n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_new[:, i], y_score[:, i])\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(i))\n",
    "\n",
    "plt.xlabel(\"false positive rate\")\n",
    "plt.ylabel(\"true positive rate\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"ROC curve for Ensemble-learning\")\n",
    "plt.show()\n",
    "print(\"ROC Score: \",mean(roc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating classification report\n",
    "print(classification_report(y_test.values,preds1,target_names=label_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating overall accuracy by checking the predicted output against the given output\n",
    "\n",
    "a=y_test.values.tolist()\n",
    "b=preds1.tolist()\n",
    "count=0\n",
    "for i in range(len(y_test)):\n",
    "    if a[i] == b[i]:\n",
    "        count+=1\n",
    "print(\"Accuracy: \",count/len(y_test)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "\n",
    "print(\"Time: \",(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
